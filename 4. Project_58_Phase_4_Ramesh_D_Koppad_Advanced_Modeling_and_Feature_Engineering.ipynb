{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "particular-hanging",
   "metadata": {},
   "source": [
    "# Advanced Modeling and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "numerous-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries required are imported\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from random import sample\n",
    "import warnings # alert the user of some condition in program\n",
    "warnings.filterwarnings('ignore') # There is warning if there are some absolete of\n",
    "                                  # certain programming elements such as keywords or class, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc7028d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To see all the columns of dataset\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb4635",
   "metadata": {},
   "source": [
    "### Import Dataset \n",
    "\n",
    "Import cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4413f2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0         a         e          i          om           w  \\\n",
      "0                0  2.769165  0.076009  10.594067   80.305532   73.597694   \n",
      "1                1  2.772466  0.230337  26.577378  173.080063  310.048857   \n",
      "2                2  2.669150  0.256942  12.988919  169.852760  248.138626   \n",
      "3                3  2.361418  0.088721   7.141771  103.810804  150.728541   \n",
      "4                4  2.574249  0.191095   5.366988  141.576605  358.687607   \n",
      "...            ...       ...       ...        ...         ...         ...   \n",
      "136401      797835  3.155975  0.343178  26.577378  115.532995  136.849398   \n",
      "136402      797860  3.171225  0.159119  26.577378  309.036573   19.746812   \n",
      "136403      798077  2.548410  0.076071  11.593237  246.298656  170.090810   \n",
      "136404      799752  3.051336  0.287449  14.456779  343.917822  342.614839   \n",
      "136405      810375  2.417477  0.109001   4.525668  148.244819   31.949854   \n",
      "\n",
      "               q        ad     per_y  data_arc  condition_code  n_obs_used  \\\n",
      "0       2.558684  2.979647  4.608202    8822.0               0      1002.0   \n",
      "1       2.133865  3.411067  4.616444   14947.5               0      2145.0   \n",
      "2       1.983332  3.354967  4.360814   14947.5               0      2145.0   \n",
      "3       2.151909  2.570926  3.628837   14947.5               0      2145.0   \n",
      "4       2.082324  3.066174  4.130323   14947.5               0      2145.0   \n",
      "...          ...       ...       ...       ...             ...         ...   \n",
      "136401  1.797805  4.372047  5.606716    2250.0               2        47.0   \n",
      "136402  2.666623  3.675826  5.647402    2373.0               1        50.0   \n",
      "136403  2.354549  2.742270  4.068291    3297.0               2        33.0   \n",
      "136404  2.174231  3.928440  5.330196    2208.0               2        27.0   \n",
      "136405  2.153970  2.680984  3.758822    3458.0               3        25.0   \n",
      "\n",
      "            H neo pha  albedo      moid class         n          per  \\\n",
      "0       11.85   N   N  0.0900  1.594780   MBA  0.213885  1683.145708   \n",
      "1       11.85   N   N  0.1010  1.233240   MBA  0.213503  1686.155999   \n",
      "2       11.85   N   N  0.2140  1.034540   MBA  0.226019  1592.787285   \n",
      "3       11.85   N   N  0.3905  1.139480   MBA  0.271609  1325.432765   \n",
      "4       11.85   N   N  0.2740  1.095890   MBA  0.238632  1508.600458   \n",
      "...       ...  ..  ..     ...       ...   ...       ...          ...   \n",
      "136401  18.20   N   N  0.1160  0.854315   MBA  0.175794  2047.852953   \n",
      "136402  16.20   N   N  0.0210  1.663010   MBA  0.174527  2062.713583   \n",
      "136403  17.30   N   N  0.0610  1.367330   MBA  0.242270  1485.943371   \n",
      "136404  17.20   N   N  0.0720  1.166840   MBA  0.184914  1946.853973   \n",
      "136405  18.40   N   N  0.0230  1.159420   MBA  0.262217  1372.909600   \n",
      "\n",
      "                ma  Diameter  \n",
      "0        77.372096    10.240  \n",
      "1        59.699133    10.240  \n",
      "2        34.925016    10.240  \n",
      "3        95.861936    10.240  \n",
      "4       282.366289    10.240  \n",
      "...            ...       ...  \n",
      "136401  195.737632     1.077  \n",
      "136402  164.999439     3.793  \n",
      "136403  145.319581     2.696  \n",
      "136404  175.708508     3.271  \n",
      "136405  170.888415     1.600  \n",
      "\n",
      "[136406 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read dataset cleaned\n",
    "dataset = pd.read_csv('Clean_Dataset.csv')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc279a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>a</th>\n",
       "      <th>e</th>\n",
       "      <th>i</th>\n",
       "      <th>om</th>\n",
       "      <th>w</th>\n",
       "      <th>q</th>\n",
       "      <th>ad</th>\n",
       "      <th>per_y</th>\n",
       "      <th>data_arc</th>\n",
       "      <th>condition_code</th>\n",
       "      <th>n_obs_used</th>\n",
       "      <th>H</th>\n",
       "      <th>neo</th>\n",
       "      <th>pha</th>\n",
       "      <th>albedo</th>\n",
       "      <th>moid</th>\n",
       "      <th>class</th>\n",
       "      <th>n</th>\n",
       "      <th>per</th>\n",
       "      <th>ma</th>\n",
       "      <th>Diameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.769165</td>\n",
       "      <td>0.076009</td>\n",
       "      <td>10.594067</td>\n",
       "      <td>80.305532</td>\n",
       "      <td>73.597694</td>\n",
       "      <td>2.558684</td>\n",
       "      <td>2.979647</td>\n",
       "      <td>4.608202</td>\n",
       "      <td>8822.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>1.59478</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.213885</td>\n",
       "      <td>1683.145708</td>\n",
       "      <td>77.372096</td>\n",
       "      <td>10.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.772466</td>\n",
       "      <td>0.230337</td>\n",
       "      <td>26.577378</td>\n",
       "      <td>173.080063</td>\n",
       "      <td>310.048857</td>\n",
       "      <td>2.133865</td>\n",
       "      <td>3.411067</td>\n",
       "      <td>4.616444</td>\n",
       "      <td>14947.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2145.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>1.23324</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.213503</td>\n",
       "      <td>1686.155999</td>\n",
       "      <td>59.699133</td>\n",
       "      <td>10.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.669150</td>\n",
       "      <td>0.256942</td>\n",
       "      <td>12.988919</td>\n",
       "      <td>169.852760</td>\n",
       "      <td>248.138626</td>\n",
       "      <td>1.983332</td>\n",
       "      <td>3.354967</td>\n",
       "      <td>4.360814</td>\n",
       "      <td>14947.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2145.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>1.03454</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.226019</td>\n",
       "      <td>1592.787285</td>\n",
       "      <td>34.925016</td>\n",
       "      <td>10.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.361418</td>\n",
       "      <td>0.088721</td>\n",
       "      <td>7.141771</td>\n",
       "      <td>103.810804</td>\n",
       "      <td>150.728541</td>\n",
       "      <td>2.151909</td>\n",
       "      <td>2.570926</td>\n",
       "      <td>3.628837</td>\n",
       "      <td>14947.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2145.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.3905</td>\n",
       "      <td>1.13948</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.271609</td>\n",
       "      <td>1325.432765</td>\n",
       "      <td>95.861936</td>\n",
       "      <td>10.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.574249</td>\n",
       "      <td>0.191095</td>\n",
       "      <td>5.366988</td>\n",
       "      <td>141.576605</td>\n",
       "      <td>358.687607</td>\n",
       "      <td>2.082324</td>\n",
       "      <td>3.066174</td>\n",
       "      <td>4.130323</td>\n",
       "      <td>14947.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2145.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.2740</td>\n",
       "      <td>1.09589</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.238632</td>\n",
       "      <td>1508.600458</td>\n",
       "      <td>282.366289</td>\n",
       "      <td>10.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         a         e          i          om           w  \\\n",
       "0           0  2.769165  0.076009  10.594067   80.305532   73.597694   \n",
       "1           1  2.772466  0.230337  26.577378  173.080063  310.048857   \n",
       "2           2  2.669150  0.256942  12.988919  169.852760  248.138626   \n",
       "3           3  2.361418  0.088721   7.141771  103.810804  150.728541   \n",
       "4           4  2.574249  0.191095   5.366988  141.576605  358.687607   \n",
       "\n",
       "          q        ad     per_y  data_arc  condition_code  n_obs_used      H  \\\n",
       "0  2.558684  2.979647  4.608202    8822.0               0      1002.0  11.85   \n",
       "1  2.133865  3.411067  4.616444   14947.5               0      2145.0  11.85   \n",
       "2  1.983332  3.354967  4.360814   14947.5               0      2145.0  11.85   \n",
       "3  2.151909  2.570926  3.628837   14947.5               0      2145.0  11.85   \n",
       "4  2.082324  3.066174  4.130323   14947.5               0      2145.0  11.85   \n",
       "\n",
       "  neo pha  albedo     moid class         n          per          ma  Diameter  \n",
       "0   N   N  0.0900  1.59478   MBA  0.213885  1683.145708   77.372096     10.24  \n",
       "1   N   N  0.1010  1.23324   MBA  0.213503  1686.155999   59.699133     10.24  \n",
       "2   N   N  0.2140  1.03454   MBA  0.226019  1592.787285   34.925016     10.24  \n",
       "3   N   N  0.3905  1.13948   MBA  0.271609  1325.432765   95.861936     10.24  \n",
       "4   N   N  0.2740  1.09589   MBA  0.238632  1508.600458  282.366289     10.24  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset.head() prints/shows first 5 rows of dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e313417f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>e</th>\n",
       "      <th>i</th>\n",
       "      <th>om</th>\n",
       "      <th>w</th>\n",
       "      <th>q</th>\n",
       "      <th>ad</th>\n",
       "      <th>per_y</th>\n",
       "      <th>data_arc</th>\n",
       "      <th>condition_code</th>\n",
       "      <th>n_obs_used</th>\n",
       "      <th>H</th>\n",
       "      <th>neo</th>\n",
       "      <th>pha</th>\n",
       "      <th>albedo</th>\n",
       "      <th>moid</th>\n",
       "      <th>class</th>\n",
       "      <th>n</th>\n",
       "      <th>per</th>\n",
       "      <th>ma</th>\n",
       "      <th>Diameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.769165</td>\n",
       "      <td>0.076009</td>\n",
       "      <td>10.594067</td>\n",
       "      <td>80.305532</td>\n",
       "      <td>73.597694</td>\n",
       "      <td>2.558684</td>\n",
       "      <td>2.979647</td>\n",
       "      <td>4.608202</td>\n",
       "      <td>8822.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>1.59478</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.213885</td>\n",
       "      <td>1683.145708</td>\n",
       "      <td>77.372096</td>\n",
       "      <td>10.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.772466</td>\n",
       "      <td>0.230337</td>\n",
       "      <td>26.577378</td>\n",
       "      <td>173.080063</td>\n",
       "      <td>310.048857</td>\n",
       "      <td>2.133865</td>\n",
       "      <td>3.411067</td>\n",
       "      <td>4.616444</td>\n",
       "      <td>14947.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2145.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>1.23324</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.213503</td>\n",
       "      <td>1686.155999</td>\n",
       "      <td>59.699133</td>\n",
       "      <td>10.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.669150</td>\n",
       "      <td>0.256942</td>\n",
       "      <td>12.988919</td>\n",
       "      <td>169.852760</td>\n",
       "      <td>248.138626</td>\n",
       "      <td>1.983332</td>\n",
       "      <td>3.354967</td>\n",
       "      <td>4.360814</td>\n",
       "      <td>14947.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2145.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>1.03454</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.226019</td>\n",
       "      <td>1592.787285</td>\n",
       "      <td>34.925016</td>\n",
       "      <td>10.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.361418</td>\n",
       "      <td>0.088721</td>\n",
       "      <td>7.141771</td>\n",
       "      <td>103.810804</td>\n",
       "      <td>150.728541</td>\n",
       "      <td>2.151909</td>\n",
       "      <td>2.570926</td>\n",
       "      <td>3.628837</td>\n",
       "      <td>14947.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2145.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.3905</td>\n",
       "      <td>1.13948</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.271609</td>\n",
       "      <td>1325.432765</td>\n",
       "      <td>95.861936</td>\n",
       "      <td>10.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.574249</td>\n",
       "      <td>0.191095</td>\n",
       "      <td>5.366988</td>\n",
       "      <td>141.576605</td>\n",
       "      <td>358.687607</td>\n",
       "      <td>2.082324</td>\n",
       "      <td>3.066174</td>\n",
       "      <td>4.130323</td>\n",
       "      <td>14947.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2145.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.2740</td>\n",
       "      <td>1.09589</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.238632</td>\n",
       "      <td>1508.600458</td>\n",
       "      <td>282.366289</td>\n",
       "      <td>10.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         e          i          om           w         q        ad  \\\n",
       "0  2.769165  0.076009  10.594067   80.305532   73.597694  2.558684  2.979647   \n",
       "1  2.772466  0.230337  26.577378  173.080063  310.048857  2.133865  3.411067   \n",
       "2  2.669150  0.256942  12.988919  169.852760  248.138626  1.983332  3.354967   \n",
       "3  2.361418  0.088721   7.141771  103.810804  150.728541  2.151909  2.570926   \n",
       "4  2.574249  0.191095   5.366988  141.576605  358.687607  2.082324  3.066174   \n",
       "\n",
       "      per_y  data_arc  condition_code  n_obs_used      H neo pha  albedo  \\\n",
       "0  4.608202    8822.0               0      1002.0  11.85   N   N  0.0900   \n",
       "1  4.616444   14947.5               0      2145.0  11.85   N   N  0.1010   \n",
       "2  4.360814   14947.5               0      2145.0  11.85   N   N  0.2140   \n",
       "3  3.628837   14947.5               0      2145.0  11.85   N   N  0.3905   \n",
       "4  4.130323   14947.5               0      2145.0  11.85   N   N  0.2740   \n",
       "\n",
       "      moid class         n          per          ma  Diameter  \n",
       "0  1.59478   MBA  0.213885  1683.145708   77.372096     10.24  \n",
       "1  1.23324   MBA  0.213503  1686.155999   59.699133     10.24  \n",
       "2  1.03454   MBA  0.226019  1592.787285   34.925016     10.24  \n",
       "3  1.13948   MBA  0.271609  1325.432765   95.861936     10.24  \n",
       "4  1.09589   MBA  0.238632  1508.600458  282.366289     10.24  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropped Unnamed column's\n",
    "dataset.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6fdb776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Data :  ['neo', 'pha', 'class']\n",
      "Numeric Data :  ['a', 'e', 'i', 'om', 'w', 'q', 'ad', 'per_y', 'data_arc', 'condition_code', 'n_obs_used', 'H', 'albedo', 'moid', 'n', 'per', 'ma', 'Diameter']\n"
     ]
    }
   ],
   "source": [
    "# Segregate/Separate the data into Numeric and Catergorical ones\n",
    "categorical, numeric = [], []\n",
    "for ele in dataset.columns:\n",
    "    if dataset[ele].dtype == 'object':\n",
    "        categorical.append(ele)\n",
    "    else:\n",
    "        numeric.append(ele)\n",
    "print(\"Categorical Data : \", categorical)\n",
    "print(\"Numeric Data : \", numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c4aaa9",
   "metadata": {},
   "source": [
    "## Converting Categorical Data into Numerical "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c11f5ce",
   "metadata": {},
   "source": [
    "### Creating Dummy Variables \n",
    "\n",
    "You can go through converting categorical data into numerical using dummy variables here: https://www.geeksforgeeks.org/convert-a-categorical-variable-into-dummy-variables/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02da7063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Data :  ['neo', 'pha', 'class']\n"
     ]
    }
   ],
   "source": [
    "print(\"Categorical Data : \", categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e826a6",
   "metadata": {},
   "source": [
    "One hot Encoder scikit-learn documentation:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "135e7003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y\n",
       "0  0\n",
       "1  0\n",
       "2  0\n",
       "3  0\n",
       "4  0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encoding columns using 'get_dummies'\n",
    "# 'Dummies': a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "# get_dummies is used for data manipulation.\n",
    "# drop_first = True :--> reduces the extra column created during dummy variable creation(Hence it reduces the correlations\n",
    "# created among dummy variables).\n",
    "dummy_neo = pd.get_dummies(dataset['neo'], drop_first = True)\n",
    "dummy_neo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "142f4db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y\n",
       "0  0\n",
       "1  0\n",
       "2  0\n",
       "3  0\n",
       "4  0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_pha = pd.get_dummies(dataset['pha'], drop_first = True)\n",
    "dummy_pha.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4c327ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APO</th>\n",
       "      <th>AST</th>\n",
       "      <th>ATE</th>\n",
       "      <th>CEN</th>\n",
       "      <th>IMB</th>\n",
       "      <th>MBA</th>\n",
       "      <th>MCA</th>\n",
       "      <th>OMB</th>\n",
       "      <th>TJN</th>\n",
       "      <th>TNO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   APO  AST  ATE  CEN  IMB  MBA  MCA  OMB  TJN  TNO\n",
       "0    0    0    0    0    0    1    0    0    0    0\n",
       "1    0    0    0    0    0    1    0    0    0    0\n",
       "2    0    0    0    0    0    1    0    0    0    0\n",
       "3    0    0    0    0    0    1    0    0    0    0\n",
       "4    0    0    0    0    0    1    0    0    0    0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_class = pd.get_dummies(dataset['class'], drop_first = True)\n",
    "dummy_class.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31cb4b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>e</th>\n",
       "      <th>i</th>\n",
       "      <th>om</th>\n",
       "      <th>w</th>\n",
       "      <th>q</th>\n",
       "      <th>ad</th>\n",
       "      <th>per_y</th>\n",
       "      <th>data_arc</th>\n",
       "      <th>condition_code</th>\n",
       "      <th>n_obs_used</th>\n",
       "      <th>H</th>\n",
       "      <th>albedo</th>\n",
       "      <th>moid</th>\n",
       "      <th>n</th>\n",
       "      <th>per</th>\n",
       "      <th>ma</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Y</th>\n",
       "      <th>Y</th>\n",
       "      <th>APO</th>\n",
       "      <th>AST</th>\n",
       "      <th>ATE</th>\n",
       "      <th>CEN</th>\n",
       "      <th>IMB</th>\n",
       "      <th>MBA</th>\n",
       "      <th>MCA</th>\n",
       "      <th>OMB</th>\n",
       "      <th>TJN</th>\n",
       "      <th>TNO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.769165</td>\n",
       "      <td>0.076009</td>\n",
       "      <td>10.594067</td>\n",
       "      <td>80.305532</td>\n",
       "      <td>73.597694</td>\n",
       "      <td>2.558684</td>\n",
       "      <td>2.979647</td>\n",
       "      <td>4.608202</td>\n",
       "      <td>8822.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>1.594780</td>\n",
       "      <td>0.213885</td>\n",
       "      <td>1683.145708</td>\n",
       "      <td>77.372096</td>\n",
       "      <td>10.240</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.772466</td>\n",
       "      <td>0.230337</td>\n",
       "      <td>26.577378</td>\n",
       "      <td>173.080063</td>\n",
       "      <td>310.048857</td>\n",
       "      <td>2.133865</td>\n",
       "      <td>3.411067</td>\n",
       "      <td>4.616444</td>\n",
       "      <td>14947.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2145.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>1.233240</td>\n",
       "      <td>0.213503</td>\n",
       "      <td>1686.155999</td>\n",
       "      <td>59.699133</td>\n",
       "      <td>10.240</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.669150</td>\n",
       "      <td>0.256942</td>\n",
       "      <td>12.988919</td>\n",
       "      <td>169.852760</td>\n",
       "      <td>248.138626</td>\n",
       "      <td>1.983332</td>\n",
       "      <td>3.354967</td>\n",
       "      <td>4.360814</td>\n",
       "      <td>14947.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2145.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>1.034540</td>\n",
       "      <td>0.226019</td>\n",
       "      <td>1592.787285</td>\n",
       "      <td>34.925016</td>\n",
       "      <td>10.240</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.361418</td>\n",
       "      <td>0.088721</td>\n",
       "      <td>7.141771</td>\n",
       "      <td>103.810804</td>\n",
       "      <td>150.728541</td>\n",
       "      <td>2.151909</td>\n",
       "      <td>2.570926</td>\n",
       "      <td>3.628837</td>\n",
       "      <td>14947.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2145.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>0.3905</td>\n",
       "      <td>1.139480</td>\n",
       "      <td>0.271609</td>\n",
       "      <td>1325.432765</td>\n",
       "      <td>95.861936</td>\n",
       "      <td>10.240</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.574249</td>\n",
       "      <td>0.191095</td>\n",
       "      <td>5.366988</td>\n",
       "      <td>141.576605</td>\n",
       "      <td>358.687607</td>\n",
       "      <td>2.082324</td>\n",
       "      <td>3.066174</td>\n",
       "      <td>4.130323</td>\n",
       "      <td>14947.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2145.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>0.2740</td>\n",
       "      <td>1.095890</td>\n",
       "      <td>0.238632</td>\n",
       "      <td>1508.600458</td>\n",
       "      <td>282.366289</td>\n",
       "      <td>10.240</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136401</th>\n",
       "      <td>3.155975</td>\n",
       "      <td>0.343178</td>\n",
       "      <td>26.577378</td>\n",
       "      <td>115.532995</td>\n",
       "      <td>136.849398</td>\n",
       "      <td>1.797805</td>\n",
       "      <td>4.372047</td>\n",
       "      <td>5.606716</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>2</td>\n",
       "      <td>47.0</td>\n",
       "      <td>18.20</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>0.854315</td>\n",
       "      <td>0.175794</td>\n",
       "      <td>2047.852953</td>\n",
       "      <td>195.737632</td>\n",
       "      <td>1.077</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136402</th>\n",
       "      <td>3.171225</td>\n",
       "      <td>0.159119</td>\n",
       "      <td>26.577378</td>\n",
       "      <td>309.036573</td>\n",
       "      <td>19.746812</td>\n",
       "      <td>2.666623</td>\n",
       "      <td>3.675826</td>\n",
       "      <td>5.647402</td>\n",
       "      <td>2373.0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>16.20</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>1.663010</td>\n",
       "      <td>0.174527</td>\n",
       "      <td>2062.713583</td>\n",
       "      <td>164.999439</td>\n",
       "      <td>3.793</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136403</th>\n",
       "      <td>2.548410</td>\n",
       "      <td>0.076071</td>\n",
       "      <td>11.593237</td>\n",
       "      <td>246.298656</td>\n",
       "      <td>170.090810</td>\n",
       "      <td>2.354549</td>\n",
       "      <td>2.742270</td>\n",
       "      <td>4.068291</td>\n",
       "      <td>3297.0</td>\n",
       "      <td>2</td>\n",
       "      <td>33.0</td>\n",
       "      <td>17.30</td>\n",
       "      <td>0.0610</td>\n",
       "      <td>1.367330</td>\n",
       "      <td>0.242270</td>\n",
       "      <td>1485.943371</td>\n",
       "      <td>145.319581</td>\n",
       "      <td>2.696</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136404</th>\n",
       "      <td>3.051336</td>\n",
       "      <td>0.287449</td>\n",
       "      <td>14.456779</td>\n",
       "      <td>343.917822</td>\n",
       "      <td>342.614839</td>\n",
       "      <td>2.174231</td>\n",
       "      <td>3.928440</td>\n",
       "      <td>5.330196</td>\n",
       "      <td>2208.0</td>\n",
       "      <td>2</td>\n",
       "      <td>27.0</td>\n",
       "      <td>17.20</td>\n",
       "      <td>0.0720</td>\n",
       "      <td>1.166840</td>\n",
       "      <td>0.184914</td>\n",
       "      <td>1946.853973</td>\n",
       "      <td>175.708508</td>\n",
       "      <td>3.271</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136405</th>\n",
       "      <td>2.417477</td>\n",
       "      <td>0.109001</td>\n",
       "      <td>4.525668</td>\n",
       "      <td>148.244819</td>\n",
       "      <td>31.949854</td>\n",
       "      <td>2.153970</td>\n",
       "      <td>2.680984</td>\n",
       "      <td>3.758822</td>\n",
       "      <td>3458.0</td>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>18.40</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>1.159420</td>\n",
       "      <td>0.262217</td>\n",
       "      <td>1372.909600</td>\n",
       "      <td>170.888415</td>\n",
       "      <td>1.600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136406 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               a         e          i          om           w         q  \\\n",
       "0       2.769165  0.076009  10.594067   80.305532   73.597694  2.558684   \n",
       "1       2.772466  0.230337  26.577378  173.080063  310.048857  2.133865   \n",
       "2       2.669150  0.256942  12.988919  169.852760  248.138626  1.983332   \n",
       "3       2.361418  0.088721   7.141771  103.810804  150.728541  2.151909   \n",
       "4       2.574249  0.191095   5.366988  141.576605  358.687607  2.082324   \n",
       "...          ...       ...        ...         ...         ...       ...   \n",
       "136401  3.155975  0.343178  26.577378  115.532995  136.849398  1.797805   \n",
       "136402  3.171225  0.159119  26.577378  309.036573   19.746812  2.666623   \n",
       "136403  2.548410  0.076071  11.593237  246.298656  170.090810  2.354549   \n",
       "136404  3.051336  0.287449  14.456779  343.917822  342.614839  2.174231   \n",
       "136405  2.417477  0.109001   4.525668  148.244819   31.949854  2.153970   \n",
       "\n",
       "              ad     per_y  data_arc  condition_code  n_obs_used      H  \\\n",
       "0       2.979647  4.608202    8822.0               0      1002.0  11.85   \n",
       "1       3.411067  4.616444   14947.5               0      2145.0  11.85   \n",
       "2       3.354967  4.360814   14947.5               0      2145.0  11.85   \n",
       "3       2.570926  3.628837   14947.5               0      2145.0  11.85   \n",
       "4       3.066174  4.130323   14947.5               0      2145.0  11.85   \n",
       "...          ...       ...       ...             ...         ...    ...   \n",
       "136401  4.372047  5.606716    2250.0               2        47.0  18.20   \n",
       "136402  3.675826  5.647402    2373.0               1        50.0  16.20   \n",
       "136403  2.742270  4.068291    3297.0               2        33.0  17.30   \n",
       "136404  3.928440  5.330196    2208.0               2        27.0  17.20   \n",
       "136405  2.680984  3.758822    3458.0               3        25.0  18.40   \n",
       "\n",
       "        albedo      moid         n          per          ma  Diameter  Y  Y  \\\n",
       "0       0.0900  1.594780  0.213885  1683.145708   77.372096    10.240  0  0   \n",
       "1       0.1010  1.233240  0.213503  1686.155999   59.699133    10.240  0  0   \n",
       "2       0.2140  1.034540  0.226019  1592.787285   34.925016    10.240  0  0   \n",
       "3       0.3905  1.139480  0.271609  1325.432765   95.861936    10.240  0  0   \n",
       "4       0.2740  1.095890  0.238632  1508.600458  282.366289    10.240  0  0   \n",
       "...        ...       ...       ...          ...         ...       ... .. ..   \n",
       "136401  0.1160  0.854315  0.175794  2047.852953  195.737632     1.077  0  0   \n",
       "136402  0.0210  1.663010  0.174527  2062.713583  164.999439     3.793  0  0   \n",
       "136403  0.0610  1.367330  0.242270  1485.943371  145.319581     2.696  0  0   \n",
       "136404  0.0720  1.166840  0.184914  1946.853973  175.708508     3.271  0  0   \n",
       "136405  0.0230  1.159420  0.262217  1372.909600  170.888415     1.600  0  0   \n",
       "\n",
       "        APO  AST  ATE  CEN  IMB  MBA  MCA  OMB  TJN  TNO  \n",
       "0         0    0    0    0    0    1    0    0    0    0  \n",
       "1         0    0    0    0    0    1    0    0    0    0  \n",
       "2         0    0    0    0    0    1    0    0    0    0  \n",
       "3         0    0    0    0    0    1    0    0    0    0  \n",
       "4         0    0    0    0    0    1    0    0    0    0  \n",
       "...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "136401    0    0    0    0    0    1    0    0    0    0  \n",
       "136402    0    0    0    0    0    1    0    0    0    0  \n",
       "136403    0    0    0    0    0    1    0    0    0    0  \n",
       "136404    0    0    0    0    0    1    0    0    0    0  \n",
       "136405    0    0    0    0    0    1    0    0    0    0  \n",
       "\n",
       "[136406 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.concat([dataset, dummy_neo, dummy_pha, dummy_class], axis=1)\n",
    "new_data.drop(['neo', 'pha', 'class'], axis=1, inplace=True)\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a88a0dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_data.drop(['Y','Y','APO','AST','ATE','CEN','IMB','MBA','MCA','OMB','TJN','TNO'])\n",
    "#new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d345b507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>e</th>\n",
       "      <th>i</th>\n",
       "      <th>om</th>\n",
       "      <th>w</th>\n",
       "      <th>q</th>\n",
       "      <th>ad</th>\n",
       "      <th>per_y</th>\n",
       "      <th>data_arc</th>\n",
       "      <th>condition_code</th>\n",
       "      <th>n_obs_used</th>\n",
       "      <th>H</th>\n",
       "      <th>albedo</th>\n",
       "      <th>moid</th>\n",
       "      <th>n</th>\n",
       "      <th>per</th>\n",
       "      <th>ma</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Y</th>\n",
       "      <th>Y</th>\n",
       "      <th>APO</th>\n",
       "      <th>AST</th>\n",
       "      <th>ATE</th>\n",
       "      <th>CEN</th>\n",
       "      <th>IMB</th>\n",
       "      <th>MBA</th>\n",
       "      <th>MCA</th>\n",
       "      <th>OMB</th>\n",
       "      <th>TJN</th>\n",
       "      <th>TNO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.769165</td>\n",
       "      <td>0.076009</td>\n",
       "      <td>10.594067</td>\n",
       "      <td>80.305532</td>\n",
       "      <td>73.597694</td>\n",
       "      <td>2.558684</td>\n",
       "      <td>2.979647</td>\n",
       "      <td>4.608202</td>\n",
       "      <td>8822.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>1.59478</td>\n",
       "      <td>0.213885</td>\n",
       "      <td>1683.145708</td>\n",
       "      <td>77.372096</td>\n",
       "      <td>10.24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.772466</td>\n",
       "      <td>0.230337</td>\n",
       "      <td>26.577378</td>\n",
       "      <td>173.080063</td>\n",
       "      <td>310.048857</td>\n",
       "      <td>2.133865</td>\n",
       "      <td>3.411067</td>\n",
       "      <td>4.616444</td>\n",
       "      <td>14947.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2145.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>1.23324</td>\n",
       "      <td>0.213503</td>\n",
       "      <td>1686.155999</td>\n",
       "      <td>59.699133</td>\n",
       "      <td>10.24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.669150</td>\n",
       "      <td>0.256942</td>\n",
       "      <td>12.988919</td>\n",
       "      <td>169.852760</td>\n",
       "      <td>248.138626</td>\n",
       "      <td>1.983332</td>\n",
       "      <td>3.354967</td>\n",
       "      <td>4.360814</td>\n",
       "      <td>14947.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2145.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>1.03454</td>\n",
       "      <td>0.226019</td>\n",
       "      <td>1592.787285</td>\n",
       "      <td>34.925016</td>\n",
       "      <td>10.24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.361418</td>\n",
       "      <td>0.088721</td>\n",
       "      <td>7.141771</td>\n",
       "      <td>103.810804</td>\n",
       "      <td>150.728541</td>\n",
       "      <td>2.151909</td>\n",
       "      <td>2.570926</td>\n",
       "      <td>3.628837</td>\n",
       "      <td>14947.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2145.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>0.3905</td>\n",
       "      <td>1.13948</td>\n",
       "      <td>0.271609</td>\n",
       "      <td>1325.432765</td>\n",
       "      <td>95.861936</td>\n",
       "      <td>10.24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.574249</td>\n",
       "      <td>0.191095</td>\n",
       "      <td>5.366988</td>\n",
       "      <td>141.576605</td>\n",
       "      <td>358.687607</td>\n",
       "      <td>2.082324</td>\n",
       "      <td>3.066174</td>\n",
       "      <td>4.130323</td>\n",
       "      <td>14947.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2145.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>0.2740</td>\n",
       "      <td>1.09589</td>\n",
       "      <td>0.238632</td>\n",
       "      <td>1508.600458</td>\n",
       "      <td>282.366289</td>\n",
       "      <td>10.24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         e          i          om           w         q        ad  \\\n",
       "0  2.769165  0.076009  10.594067   80.305532   73.597694  2.558684  2.979647   \n",
       "1  2.772466  0.230337  26.577378  173.080063  310.048857  2.133865  3.411067   \n",
       "2  2.669150  0.256942  12.988919  169.852760  248.138626  1.983332  3.354967   \n",
       "3  2.361418  0.088721   7.141771  103.810804  150.728541  2.151909  2.570926   \n",
       "4  2.574249  0.191095   5.366988  141.576605  358.687607  2.082324  3.066174   \n",
       "\n",
       "      per_y  data_arc  condition_code  n_obs_used      H  albedo     moid  \\\n",
       "0  4.608202    8822.0               0      1002.0  11.85  0.0900  1.59478   \n",
       "1  4.616444   14947.5               0      2145.0  11.85  0.1010  1.23324   \n",
       "2  4.360814   14947.5               0      2145.0  11.85  0.2140  1.03454   \n",
       "3  3.628837   14947.5               0      2145.0  11.85  0.3905  1.13948   \n",
       "4  4.130323   14947.5               0      2145.0  11.85  0.2740  1.09589   \n",
       "\n",
       "          n          per          ma  Diameter  Y  Y  APO  AST  ATE  CEN  IMB  \\\n",
       "0  0.213885  1683.145708   77.372096     10.24  0  0    0    0    0    0    0   \n",
       "1  0.213503  1686.155999   59.699133     10.24  0  0    0    0    0    0    0   \n",
       "2  0.226019  1592.787285   34.925016     10.24  0  0    0    0    0    0    0   \n",
       "3  0.271609  1325.432765   95.861936     10.24  0  0    0    0    0    0    0   \n",
       "4  0.238632  1508.600458  282.366289     10.24  0  0    0    0    0    0    0   \n",
       "\n",
       "   MBA  MCA  OMB  TJN  TNO  \n",
       "0    1    0    0    0    0  \n",
       "1    1    0    0    0    0  \n",
       "2    1    0    0    0    0  \n",
       "3    1    0    0    0    0  \n",
       "4    1    0    0    0    0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ae5a6",
   "metadata": {},
   "source": [
    "## Split data into Train and Test Data \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "you can go through this here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6269287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing and defining the model\n",
    "# \"sklearn.model_selection.train_test_split\": Split arrays or matrices into random train and test subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_data.drop('Diameter', axis=1), new_data['Diameter'], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038d06da",
   "metadata": {},
   "source": [
    "### Scaling the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad2b09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing and defining model\n",
    "# \"sklearn.preprocessing import StandardScaler\": Standardize features by removing the mean and scaling to unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler()\n",
    "\n",
    "# \"scale.fit_transform\" \n",
    "X_train = scale.fit_transform(X_train)\n",
    "\n",
    "#\"scale.transform\"\n",
    "X_test = scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fcf72f",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "\n",
    "You can go through all these here:\n",
    "\n",
    "\"from sklearn.metrics import mean_absolute_error\": https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html\n",
    "\n",
    "\"from sklearn.metrics import mean_squared_error\": https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\n",
    "\n",
    "\"from sklearn.metrics import r2_score\": https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html\n",
    "\n",
    "\"from math import sqrt\": https://www.geeksforgeeks.org/python-math-function-sqrt/\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d460b4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from math import sqrt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dba06e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_pred, y_actual):\n",
    "    mae = mean_absolute_error(y_actual, y_pred)\n",
    "    mse = mean_squared_error(y_actual, y_pred)\n",
    "    rmse = sqrt(mse)\n",
    "    r2 = r2_score(y_actual, y_pred)\n",
    "    \n",
    "    print(\"Mean Absolute Error :->\", mae)\n",
    "    print(\"Mean Squared Error :->\", mse)\n",
    "    print(\"Root Mean Squared Error :->\", rmse)\n",
    "    print(\"R2_Score :->\", r2)\n",
    "    \n",
    "    return mae, mse, rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9202596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Applying Algorithms\"\n",
    "# for storing algorithms name and it's performanace\n",
    "algo_score = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea0a1a3",
   "metadata": {},
   "source": [
    "## Applying Algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436e79d",
   "metadata": {},
   "source": [
    "## Random Forest Regressor\n",
    "\n",
    "sklearn.ensemble.RandomForestRegressor\n",
    "\n",
    "you can go through this here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43a01f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the random forest regressor model on the whole dataset\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RF_regressor = RandomForestRegressor()\n",
    "RF_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65e0d680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :-> 0.26871852943333013\n",
      "Mean Squared Error :-> 0.17632615109115907\n",
      "Root Mean Squared Error :-> 0.41991207542908204\n",
      "R2_Score :-> 0.969623951427643\n"
     ]
    }
   ],
   "source": [
    "# predicting the test set results\n",
    "y_pred_RF = RF_regressor.predict(X_test)\n",
    "mae, mse, rmse, r2 = evaluate(y_test, y_pred_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4c145ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_score['Random Forest Regressor'] = r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c362faf",
   "metadata": {},
   "source": [
    "## K Nearest Neighbor Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa988b42",
   "metadata": {},
   "source": [
    "sklearn.neighbors.KNeighborsRegressor\n",
    "\n",
    "You can go through this here: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "835d6674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(n_neighbors=4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "KNN = KNeighborsRegressor(n_neighbors = 4)\n",
    "KNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "614e1b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :-> 0.4779474103804707\n",
      "Mean Squared Error :-> 0.4730649601683345\n",
      "Root Mean Squared Error :-> 0.6877971795292087\n",
      "R2_Score :-> 0.9116504039294485\n"
     ]
    }
   ],
   "source": [
    "# predicting the test set results\n",
    "y_pred_KNN = KNN.predict(X_test)\n",
    "mae, mse, rmse, r2 = evaluate(y_test, y_pred_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0655477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_score['K Nearest Neighbor Regressor'] = r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f1141",
   "metadata": {},
   "source": [
    "## Linear Regression \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "you can go through this here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43efcb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the linear regressor model on the whole dataset\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linear_regression = LinearRegression()\n",
    "\n",
    "# training\n",
    "linear_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9eaddf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :-> 0.5988856865940341\n",
      "Mean Squared Error :-> 0.6338630346133242\n",
      "Root Mean Squared Error :-> 0.7961551573740663\n",
      "R2_Score :-> 0.8812456820746711\n"
     ]
    }
   ],
   "source": [
    "# predicting the test set results\n",
    "y_pred_LR = linear_regression.predict(X_test)\n",
    "mae, mse, rmse, r2 = evaluate(y_test, y_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "440353f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_score['Linear Regression'] = r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba65120",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor \n",
    "\n",
    "sklearn.tree.DecisionTreeRegressor\n",
    "\n",
    "you can go through this here: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "739f8131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the decision tree regressor model on the whole dataset\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "DT = DecisionTreeRegressor()\n",
    "DT.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e46345d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :-> 0.38444461549741255\n",
      "Mean Squared Error :-> 0.35866529733890484\n",
      "Root Mean Squared Error :-> 0.5988867149460779\n",
      "R2_Score :-> 0.9400653150767918\n"
     ]
    }
   ],
   "source": [
    "# predicting the test set results\n",
    "y_pred_DT = DT.predict(X_test)\n",
    "mae, mse, rmse, r2 = evaluate(y_test, y_pred_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af8a8bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_score['Decision Tree Regressor'] = r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc369e",
   "metadata": {},
   "source": [
    "## XG Boost Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f09f19",
   "metadata": {},
   "source": [
    "You can go through this here: https://xgboost.readthedocs.io/en/stable/tutorials/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b68180d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "             gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.300000012,\n",
       "             max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=100, n_jobs=4,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "XGB = XGBRegressor()\n",
    "XGB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "860e1728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :-> 0.275480797861573\n",
      "Mean Squared Error :-> 0.17380856857402793\n",
      "Root Mean Squared Error :-> 0.4169035482866846\n",
      "R2_Score :-> 0.9703281842649443\n"
     ]
    }
   ],
   "source": [
    "# predicting the test set results\n",
    "y_pred_XGB = XGB.predict(X_test)\n",
    "mae, mse, rmse, r2 = evaluate(y_test, y_pred_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b24e28f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_score['XG Boost Regressor'] = r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b431b684",
   "metadata": {},
   "source": [
    "## XG Boost with Hyperparameter Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "471794f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Hyperparameters\n",
    "hyper_parameter = {'learning_rate':[0.290, 0.30, 0.301],\n",
    "                  'max_depth':[4, 6, 8],\n",
    "                  'min_child_weight':[1, 3],\n",
    "                  'gamma':[1, 0.1, 0.1],\n",
    "                  'colsample_bytree':[0.9, 1, 1.1],\n",
    "                  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b1eda4",
   "metadata": {},
   "source": [
    "### Using RandomizedSearchCV for finding the best parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2afb04ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(XGB, param_distributions = hyper_parameter, n_jobs = -1, scoring = \"neg_mean_squared_error\", cv = 4, verbose = 3, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f26d7492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4,\n",
       "                   estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                          colsample_bylevel=1,\n",
       "                                          colsample_bynode=1,\n",
       "                                          colsample_bytree=1,\n",
       "                                          enable_categorical=False, gamma=0,\n",
       "                                          gpu_id=-1, importance_type=None,\n",
       "                                          interaction_constraints='',\n",
       "                                          learning_rate=0.300000012,\n",
       "                                          max_delta_step=0, max_depth=6,\n",
       "                                          min_child_weight=1, missing=nan,\n",
       "                                          monotone_constraints='()',\n",
       "                                          n_estimato...\n",
       "                                          num_parallel_tree=1, predictor='auto',\n",
       "                                          random_state=0, reg_alpha=0,\n",
       "                                          reg_lambda=1, scale_pos_weight=1,\n",
       "                                          subsample=1, tree_method='exact',\n",
       "                                          validate_parameters=1,\n",
       "                                          verbosity=None),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.9, 1, 1.1],\n",
       "                                        'gamma': [1, 0.1, 0.1],\n",
       "                                        'learning_rate': [0.29, 0.3, 0.301],\n",
       "                                        'max_depth': [4, 6, 8],\n",
       "                                        'min_child_weight': [1, 3]},\n",
       "                   random_state=10, scoring='neg_mean_squared_error',\n",
       "                   verbose=3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b3bdfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters found are : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'min_child_weight': 1,\n",
       " 'max_depth': 8,\n",
       " 'learning_rate': 0.29,\n",
       " 'gamma': 1,\n",
       " 'colsample_bytree': 1}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best Parameters found are : \")\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca3b56d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :-> 0.26802133410794393\n",
      "Mean Squared Error :-> 0.16979092180902614\n",
      "Root Mean Squared Error :-> 0.41205694000832715\n",
      "R2_Score :-> 0.9709879382816159\n"
     ]
    }
   ],
   "source": [
    "# predicting the test set results\n",
    "y_pred_XGB_Opt = random_search.predict(X_test)\n",
    "mae, mse, rmse, r2 = evaluate(y_test, y_pred_XGB_Opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dce6e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_score['XG Boost Optimizer'] = r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47f3e81",
   "metadata": {},
   "source": [
    "## Artificial Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14159c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0034c692",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(input_dim = X_train.shape[1], units = 12, kernel_initializer = \"he_uniform\", activation = \"relu\"))\n",
    "model.add(Dense(units = 10, kernel_initializer = \"he_uniform\", activation = \"relu\"))\n",
    "model.add(Dense(units = 1, activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "07a71cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 12)                360       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                130       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 501\n",
      "Trainable params: 501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8202a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7278c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = np.asarray(X_train).astype(np.float32)\n",
    "#y_train = np.asarray(y_train).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ce7fc90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "764/764 [==============================] - 6s 4ms/step - loss: 3.7408 - val_loss: 0.7774\n",
      "Epoch 2/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.7879 - val_loss: 0.4941\n",
      "Epoch 3/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.5050 - val_loss: 0.4227\n",
      "Epoch 4/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.4236 - val_loss: 0.3844\n",
      "Epoch 5/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.3744 - val_loss: 0.3498\n",
      "Epoch 6/200\n",
      "764/764 [==============================] - 3s 3ms/step - loss: 0.3389 - val_loss: 0.3170\n",
      "Epoch 7/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.3076 - val_loss: 0.2870\n",
      "Epoch 8/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2825 - val_loss: 0.2657\n",
      "Epoch 9/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2644 - val_loss: 0.2507\n",
      "Epoch 10/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2522 - val_loss: 0.2401\n",
      "Epoch 11/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2433 - val_loss: 0.2326\n",
      "Epoch 12/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2381 - val_loss: 0.2284\n",
      "Epoch 13/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2319 - val_loss: 0.2250\n",
      "Epoch 14/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2285 - val_loss: 0.2213\n",
      "Epoch 15/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2261 - val_loss: 0.2161\n",
      "Epoch 16/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2241 - val_loss: 0.2182\n",
      "Epoch 17/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2215 - val_loss: 0.2185\n",
      "Epoch 18/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2206 - val_loss: 0.2116\n",
      "Epoch 19/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2183 - val_loss: 0.2155\n",
      "Epoch 20/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2174 - val_loss: 0.2151\n",
      "Epoch 21/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2136 - val_loss: 0.2055\n",
      "Epoch 22/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2050 - val_loss: 0.2103\n",
      "Epoch 23/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2041 - val_loss: 0.2017\n",
      "Epoch 24/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2033 - val_loss: 0.2022\n",
      "Epoch 25/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2018 - val_loss: 0.2013\n",
      "Epoch 26/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2020 - val_loss: 0.1986\n",
      "Epoch 27/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2008 - val_loss: 0.1990\n",
      "Epoch 28/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2005 - val_loss: 0.2005\n",
      "Epoch 29/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1991 - val_loss: 0.1990\n",
      "Epoch 30/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1991 - val_loss: 0.1989\n",
      "Epoch 31/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1983 - val_loss: 0.1968\n",
      "Epoch 32/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1979 - val_loss: 0.1945\n",
      "Epoch 33/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1973 - val_loss: 0.1973\n",
      "Epoch 34/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1982 - val_loss: 0.1946\n",
      "Epoch 35/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1963 - val_loss: 0.2018\n",
      "Epoch 36/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1960 - val_loss: 0.1971\n",
      "Epoch 37/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1956 - val_loss: 0.1961\n",
      "Epoch 38/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1961 - val_loss: 0.1952\n",
      "Epoch 39/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1961 - val_loss: 0.1976\n",
      "Epoch 40/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1955 - val_loss: 0.1968\n",
      "Epoch 41/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1954 - val_loss: 0.1960\n",
      "Epoch 42/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1950 - val_loss: 0.2005\n",
      "Epoch 43/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1945 - val_loss: 0.1927\n",
      "Epoch 44/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1950 - val_loss: 0.1925\n",
      "Epoch 45/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1949 - val_loss: 0.1951\n",
      "Epoch 46/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1946 - val_loss: 0.1946\n",
      "Epoch 47/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1939 - val_loss: 0.1971\n",
      "Epoch 48/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1943 - val_loss: 0.1940\n",
      "Epoch 49/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1936 - val_loss: 0.1946\n",
      "Epoch 50/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1940 - val_loss: 0.1955\n",
      "Epoch 51/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1929 - val_loss: 0.1920\n",
      "Epoch 52/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1933 - val_loss: 0.1949\n",
      "Epoch 53/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1927 - val_loss: 0.1950\n",
      "Epoch 54/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1929 - val_loss: 0.1932\n",
      "Epoch 55/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1926 - val_loss: 0.1931\n",
      "Epoch 56/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1925 - val_loss: 0.1918\n",
      "Epoch 57/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1922 - val_loss: 0.1922\n",
      "Epoch 58/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1928 - val_loss: 0.1910\n",
      "Epoch 59/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1924 - val_loss: 0.1929\n",
      "Epoch 60/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1921 - val_loss: 0.1948\n",
      "Epoch 61/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1922 - val_loss: 0.1925\n",
      "Epoch 62/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1923 - val_loss: 0.1939\n",
      "Epoch 63/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1922 - val_loss: 0.1934\n",
      "Epoch 64/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1919 - val_loss: 0.1931\n",
      "Epoch 65/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1913 - val_loss: 0.1920\n",
      "Epoch 66/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1920 - val_loss: 0.1923\n",
      "Epoch 67/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1915 - val_loss: 0.1914\n",
      "Epoch 68/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1915 - val_loss: 0.1914\n",
      "Epoch 69/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1915 - val_loss: 0.1956\n",
      "Epoch 70/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1906 - val_loss: 0.1937\n",
      "Epoch 71/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1908 - val_loss: 0.1929\n",
      "Epoch 72/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1915 - val_loss: 0.1943\n",
      "Epoch 73/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1906 - val_loss: 0.1945\n",
      "Epoch 74/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1909 - val_loss: 0.1953\n",
      "Epoch 75/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1906 - val_loss: 0.1920\n",
      "Epoch 76/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1904 - val_loss: 0.1930\n",
      "Epoch 77/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1908 - val_loss: 0.1925\n",
      "Epoch 78/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1903 - val_loss: 0.1899\n",
      "Epoch 79/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1904 - val_loss: 0.1886\n",
      "Epoch 80/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1899 - val_loss: 0.1916\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1898 - val_loss: 0.1925\n",
      "Epoch 82/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1902 - val_loss: 0.1930\n",
      "Epoch 83/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1896 - val_loss: 0.1924\n",
      "Epoch 84/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1902 - val_loss: 0.1907\n",
      "Epoch 85/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1899 - val_loss: 0.1901\n",
      "Epoch 86/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1897 - val_loss: 0.1914\n",
      "Epoch 87/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1890 - val_loss: 0.1901\n",
      "Epoch 88/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1899 - val_loss: 0.1880\n",
      "Epoch 89/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1903 - val_loss: 0.1926\n",
      "Epoch 90/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1898 - val_loss: 0.1892\n",
      "Epoch 91/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1890 - val_loss: 0.1927\n",
      "Epoch 92/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1892 - val_loss: 0.1913\n",
      "Epoch 93/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1894 - val_loss: 0.1897\n",
      "Epoch 94/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1896 - val_loss: 0.1905\n",
      "Epoch 95/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1895 - val_loss: 0.1889\n",
      "Epoch 96/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1891 - val_loss: 0.1924\n",
      "Epoch 97/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1889 - val_loss: 0.1923\n",
      "Epoch 98/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1892 - val_loss: 0.1929\n",
      "Epoch 99/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1884 - val_loss: 0.1899\n",
      "Epoch 100/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1886 - val_loss: 0.1905\n",
      "Epoch 101/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1890 - val_loss: 0.1936\n",
      "Epoch 102/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1887 - val_loss: 0.1894\n",
      "Epoch 103/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1891 - val_loss: 0.1922\n",
      "Epoch 104/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1888 - val_loss: 0.1905\n",
      "Epoch 105/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1888 - val_loss: 0.1878\n",
      "Epoch 106/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1886 - val_loss: 0.1938\n",
      "Epoch 107/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1885 - val_loss: 0.1916\n",
      "Epoch 108/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1884 - val_loss: 0.1901\n",
      "Epoch 109/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1883 - val_loss: 0.1898\n",
      "Epoch 110/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1890 - val_loss: 0.1883\n",
      "Epoch 111/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1887 - val_loss: 0.1956\n",
      "Epoch 112/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1891 - val_loss: 0.1896\n",
      "Epoch 113/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1878 - val_loss: 0.1889\n",
      "Epoch 114/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1883 - val_loss: 0.1918\n",
      "Epoch 115/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1880 - val_loss: 0.1897\n",
      "Epoch 116/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1881 - val_loss: 0.1922\n",
      "Epoch 117/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1891 - val_loss: 0.1918\n",
      "Epoch 118/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1880 - val_loss: 0.1906\n",
      "Epoch 119/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1881 - val_loss: 0.1907\n",
      "Epoch 120/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1889 - val_loss: 0.1880\n",
      "Epoch 121/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1888 - val_loss: 0.1939\n",
      "Epoch 122/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1881 - val_loss: 0.1933\n",
      "Epoch 123/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1878 - val_loss: 0.1895\n",
      "Epoch 124/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1878 - val_loss: 0.1883\n",
      "Epoch 125/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1886 - val_loss: 0.1922\n",
      "Epoch 126/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1884 - val_loss: 0.1894\n",
      "Epoch 127/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1881 - val_loss: 0.1930\n",
      "Epoch 128/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1877 - val_loss: 0.1893\n",
      "Epoch 129/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1880 - val_loss: 0.1886\n",
      "Epoch 130/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1880 - val_loss: 0.1888\n",
      "Epoch 131/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1874 - val_loss: 0.1895\n",
      "Epoch 132/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1878 - val_loss: 0.1887\n",
      "Epoch 133/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1879 - val_loss: 0.1893\n",
      "Epoch 134/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1871 - val_loss: 0.1892\n",
      "Epoch 135/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1880 - val_loss: 0.1893\n",
      "Epoch 136/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1875 - val_loss: 0.1911\n",
      "Epoch 137/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1880 - val_loss: 0.1891\n",
      "Epoch 138/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1877 - val_loss: 0.1877\n",
      "Epoch 139/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1875 - val_loss: 0.1883\n",
      "Epoch 140/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1876 - val_loss: 0.1876\n",
      "Epoch 141/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1879 - val_loss: 0.1898\n",
      "Epoch 142/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1877 - val_loss: 0.1893\n",
      "Epoch 143/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1872 - val_loss: 0.1926\n",
      "Epoch 144/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1878 - val_loss: 0.1880\n",
      "Epoch 145/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1873 - val_loss: 0.1889\n",
      "Epoch 146/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1874 - val_loss: 0.1883\n",
      "Epoch 147/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1875 - val_loss: 0.1884\n",
      "Epoch 148/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1873 - val_loss: 0.1893\n",
      "Epoch 149/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1878 - val_loss: 0.1879\n",
      "Epoch 150/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1868 - val_loss: 0.1895\n",
      "Epoch 151/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1873 - val_loss: 0.1884\n",
      "Epoch 152/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1878 - val_loss: 0.1890\n",
      "Epoch 153/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1869 - val_loss: 0.1881\n",
      "Epoch 154/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1876 - val_loss: 0.1882\n",
      "Epoch 155/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1869 - val_loss: 0.1893\n",
      "Epoch 156/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1873 - val_loss: 0.1903\n",
      "Epoch 157/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1869 - val_loss: 0.1888\n",
      "Epoch 158/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1875 - val_loss: 0.1890\n",
      "Epoch 159/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1872 - val_loss: 0.1900\n",
      "Epoch 160/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1868 - val_loss: 0.1901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1865 - val_loss: 0.1907\n",
      "Epoch 162/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1872 - val_loss: 0.1903\n",
      "Epoch 163/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1868 - val_loss: 0.1885\n",
      "Epoch 164/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1867 - val_loss: 0.1885\n",
      "Epoch 165/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1867 - val_loss: 0.1896\n",
      "Epoch 166/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1867 - val_loss: 0.1899\n",
      "Epoch 167/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1865 - val_loss: 0.1895\n",
      "Epoch 168/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1867 - val_loss: 0.1970\n",
      "Epoch 169/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1871 - val_loss: 0.1874\n",
      "Epoch 170/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1868 - val_loss: 0.1875\n",
      "Epoch 171/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1871 - val_loss: 0.1919\n",
      "Epoch 172/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1866 - val_loss: 0.1879\n",
      "Epoch 173/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1862 - val_loss: 0.1879\n",
      "Epoch 174/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1865 - val_loss: 0.1872\n",
      "Epoch 175/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1865 - val_loss: 0.1900\n",
      "Epoch 176/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1858 - val_loss: 0.1888\n",
      "Epoch 177/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1859 - val_loss: 0.1899\n",
      "Epoch 178/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1863 - val_loss: 0.1864\n",
      "Epoch 179/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1865 - val_loss: 0.1890\n",
      "Epoch 180/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1865 - val_loss: 0.1879\n",
      "Epoch 181/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1870 - val_loss: 0.1878\n",
      "Epoch 182/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1867 - val_loss: 0.1899\n",
      "Epoch 183/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1863 - val_loss: 0.1908\n",
      "Epoch 184/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1868 - val_loss: 0.1907\n",
      "Epoch 185/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1860 - val_loss: 0.1892\n",
      "Epoch 186/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1862 - val_loss: 0.1904\n",
      "Epoch 187/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1866 - val_loss: 0.1884\n",
      "Epoch 188/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1863 - val_loss: 0.1879\n",
      "Epoch 189/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1859 - val_loss: 0.1876\n",
      "Epoch 190/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1861 - val_loss: 0.1882\n",
      "Epoch 191/200\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.1862 - val_loss: 0.1866\n",
      "Epoch 192/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1863 - val_loss: 0.1893\n",
      "Epoch 193/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1860 - val_loss: 0.1881\n",
      "Epoch 194/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1862 - val_loss: 0.1873\n",
      "Epoch 195/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1862 - val_loss: 0.1877\n",
      "Epoch 196/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1859 - val_loss: 0.1928\n",
      "Epoch 197/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1866 - val_loss: 0.1889\n",
      "Epoch 198/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1864 - val_loss: 0.1896\n",
      "Epoch 199/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1856 - val_loss: 0.1886\n",
      "Epoch 200/200\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1853 - val_loss: 0.1892\n"
     ]
    }
   ],
   "source": [
    "History = model.fit(X_train, y_train, validation_split = 0.3, epochs = 200, batch_size = 100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "576b9a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :-> 0.2891273109701214\n",
      "Mean Squared Error :-> 0.18918135229726651\n",
      "Root Mean Squared Error :-> 0.4349498273332989\n",
      "R2_Score :-> 0.9670964102180357\n"
     ]
    }
   ],
   "source": [
    "# predicting the test set results\n",
    "y_pred_model = model.predict(X_test)\n",
    "mae, mse, rmse, r2 = evaluate(y_test, y_pred_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f03b47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_score['Artificial Neural Network'] = r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7564ac",
   "metadata": {},
   "source": [
    "## Accuracy of Algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dabb61",
   "metadata": {},
   "source": [
    "Designed 'Algorithm' VS 'R_squared' barplot for visualization and comparing the performance of algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49acd2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bar plot of different regressor algorithms\n",
    "# plotted between Algorithms Vs R-Squared Values\n",
    "plt.figure(figsize = (20, 8))\n",
    "sns.barplot(x = list(algo_score.keys()), y = list(algo_score.values()))\n",
    "plt.title(\"Algorithms Performance\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "plt.ylabel(\"R-Squared Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "71b6978f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor ---> 0.969623951427643\n",
      "K Nearest Neighbor Regressor ---> 0.9116504039294485\n",
      "Linear Regression ---> 0.8812456820746711\n",
      "Decision Tree Regressor ---> 0.9400653150767918\n",
      "XG Boost Regressor ---> 0.9703281842649443\n",
      "XG Boost Optimizer ---> 0.9709879382816159\n",
      "Artificial Neural Network ---> 0.9670964102180357\n"
     ]
    }
   ],
   "source": [
    "# Printing all the regressor algorithms and their R-squared values from the output/results obtained\n",
    "for key, val in algo_score.items():\n",
    "    print(key, '--->', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8adcaa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+---------------------+\n",
      "|          Algorithm           |   R-Squared values  |\n",
      "+------------------------------+---------------------+\n",
      "|   Random Forest Regressor    |  0.969623951427643  |\n",
      "| K Nearest Neighbor Regressor |  0.9116504039294485 |\n",
      "|      Linear Regression       |  0.8812456820746711 |\n",
      "|   Decision Tree Regressor    |  0.9400653150767918 |\n",
      "|      XG Boost Regressor      |  0.9703281842649443 |\n",
      "|      XG Boost Optimizer      |  0.9709879382816159 |\n",
      "|  Artificial Neural Network   |  0.9670964102180357 |\n",
      "+------------------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "  \n",
    "# Specify the Column Names while initializing the Table\n",
    "myTable = PrettyTable([\"Algorithm\", \"R-Squared values\"])\n",
    "  \n",
    "# Add rows\n",
    "myTable.add_row([\"Random Forest Regressor\",\"0.969623951427643\"])\n",
    "myTable.add_row([\"K Nearest Neighbor Regressor\", \"0.9116504039294485\"])\n",
    "myTable.add_row([\"Linear Regression\", \" 0.8812456820746711\"])\n",
    "myTable.add_row([\"Decision Tree Regressor\", \"0.9400653150767918\"])\n",
    "myTable.add_row([\"XG Boost Regressor\", \"0.9703281842649443\"])\n",
    "myTable.add_row([\"XG Boost Optimizer\", \"0.9709879382816159\"])\n",
    "myTable.add_row([\"Artificial Neural Network\", \"0.9670964102180357\"])\n",
    "  \n",
    "print(myTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "abf434d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Performing Algorithm is :--->  XG Boost Optimizer\n",
      "R-Squared :--->  0.9709879382816159\n"
     ]
    }
   ],
   "source": [
    "# finding the best performing regressor algorithm and its R-squared value\n",
    "maxi = 0\n",
    "best_algo = \"\"\n",
    "for key, val in algo_score.items():\n",
    "    if val > maxi:\n",
    "        maxi = val\n",
    "        best_algo = key\n",
    "print(\"Best Performing Algorithm is :---> \", best_algo)\n",
    "print(\"R-Squared :---> \", maxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c97d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
